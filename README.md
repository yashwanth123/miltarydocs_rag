# ğŸª– MilitaryDocs RAG

**MilitaryDocs RAG** is a Retrieval-Augmented Generation (RAG) application built using **FastAPI**, **Pinecone**, **OpenAI**, and **LangChain**, designed to allow semantic search and question answering over large collections of scanned military documents (PDFs).

---

## ğŸš€ Features

- âœ… Ingests scanned PDFs with OCR (Tesseract + PyMuPDF)
- âœ… Extracts text and splits into context-aware chunks
- âœ… Stores vector embeddings in **Pinecone**
- âœ… Retrieves relevant document chunks using LangChain retriever
- âœ… Uses GPT-4 (or Claude) to generate answers grounded in PDF content
- âœ… Exposed via a **FastAPI backend** and a basic **HTML frontend**

---

## ğŸ“ Project Structure

```bash
miltarydocs_rag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ endpoints.py         # API routes
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ embedding_service.py # Main query logic
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ chunker.py           # Text chunking
â”‚   â”‚   â””â”€â”€ pdf_parser.py        # OCR + PDF parsing
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â””â”€â”€ pinecone_client.py   # Pinecone vector DB integration
â”‚   â””â”€â”€ main.py                  # FastAPI entrypoint
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest_documents.py      # Script to process + embed PDFs
â”œâ”€â”€ data/                        # Place your PDFs here
â”œâ”€â”€ .env                         # API keys and configs
â””â”€â”€ requirements.txt


ğŸ”§ Setup Instructions
1. Clone & Set Up Environment
git clone https://github.com/yourusername/militarydocs_rag.git
cd militarydocs_rag
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

2. Install OCR Dependencies
brew install poppler tesseract

3. Create .env File
OPENAI_API_KEY=sk-xxxx
PINECONE_API_KEY=pcsk-xxxx
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX=military-docs

ğŸ“¥ Ingest PDFs (with OCR)
Place your scanned or native PDFs inside data/, e.g.:


data/pdf1.pdf
data/manual_2024.pdf

Run ingestion:
export PYTHONPATH=.
python scripts/ingest_documents.py

ğŸŒ Start the API Server
uvicorn backend.main:app --reload
Then open: http://127.0.0.1:8000/docs

ğŸ§  How It Works (RAG Flow)
OCR with PyMuPDF + Tesseract â†’ extracts clean text from scanned PDFs

Text Chunking â†’ splits long documents into semantic blocks

Embedding â†’ uses OpenAI's embedding model (text-embedding-3-small)

Pinecone â†’ stores and indexes embeddings for similarity search

LangChain Retriever â†’ pulls relevant chunks for any user query

LLM â†’ GPT-4 (or Claude) generates a grounded response

FastAPI Endpoint â†’ delivers results via REST API

âœ… Example API Usage
POST /ask

json
Copy
Edit
{
  "query": "What are the procedures for emergency deployment?",
  "top_k": 5
}
Response:

json
Copy
Edit
{
  "answer": "According to the manual, emergency deployment involves..."
}
ğŸ“ˆ Next Steps
Task	Status
âœ… OCR for scanned PDFs	Done
âœ… FastAPI backend with query endpoint	Done
âœ… Pinecone + OpenAI integration	Done
âŒ Switch to HuggingFace embeddings (optional)	ğŸ”œ
âŒ Web frontend (HTML input + results)	ğŸ”œ
âŒ Auth & document upload portal (multi-user)	ğŸ”œ
âŒ Cost monitoring & rate-limit handling	ğŸ”œ
âŒ Deploy to Render / EC2	ğŸ”œ

ğŸ’¡ Tips
Want to avoid OpenAI costs while developing? Swap embeddings for all-MiniLM-L6-v2 from HuggingFace (I can help you do this).

Use tiktoken to estimate chunk token lengths for better LLM context handling.

Consider replacing deprecated LangchainPinecone with PineconeVectorStore.

ğŸ¤ Credits
Built with â¤ï¸ using:

FastAPI

LangChain

OpenAI

Pinecone

pdfminer.six

PyMuPDF (fitz)

Tesseract OCR

ğŸ“¬ Contact
Yashwanth Sai Tirukkovalluru
Feel free to open issues or contact for deployment & scaling help.


